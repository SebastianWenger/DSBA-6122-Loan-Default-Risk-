{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.utils import resample\n",
    "import featuretools as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set random number seed and import main table in hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=123\n",
    "\n",
    "APPLICATION_TRAIN = pd.read_csv(\"application_train.csv\",na_values=['XNA','XAP'])\n",
    "APPLICATION_TRAIN['DAYS_EMPLOYED'].replace(365243,np.nan,inplace=True)\n",
    "\n",
    "APPLICATION_TRAIN['AMT_GOODS_PRICE-AMT_CREDIT']=APPLICATION_TRAIN['AMT_GOODS_PRICE']-APPLICATION_TRAIN['AMT_CREDIT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsample using resample function imported above.  Not enough memory to use an oversampled dataset and minority class is large enough.  \n",
    "\n",
    "## We declare \"keys\" variable (loan ids of the downsampled data), which we will use to filter the sub tables on later to lower memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_majority=APPLICATION_TRAIN[APPLICATION_TRAIN['TARGET']==0]\n",
    "df_minority=APPLICATION_TRAIN[APPLICATION_TRAIN['TARGET']==1]\n",
    "\n",
    "df_majority_downsampled = resample(df_majority,replace=False,n_samples=len(df_minority),random_state=seed) \n",
    "\n",
    "APPLICATION_TRAIN = pd.concat([df_majority_downsampled, df_minority]).reset_index(drop=True)\n",
    "\n",
    "keys=APPLICATION_TRAIN.SK_ID_CURR\n",
    "\n",
    "APPLICATION_TRAIN=APPLICATION_TRAIN.set_index('SK_ID_CURR')\n",
    "\n",
    "del df_majority,df_minority,df_majority_downsampled; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"generate_interactions\" - function to create all pairwise combinations of *,/,-,+ for all integer and float datatype columns.  \n",
    "\n",
    "## \"correlated_columns\" - function that returns all column names in a dataframe to drop that will result in no 2 columns having >.95 pearson correlation with each other. Column with more NAs is dropped.\n",
    "\n",
    "## \"aggregate_category_freq\" - function that aggregates categorical variables onto a main table, creating dummy variables with each column value being the proportion of occurence for the unique id. \n",
    "\n",
    "## These functions are necessary to declare early on as many correlated interaction terms  are created, and identifying early on and dropping is essential for memory usage.  \n",
    "\n",
    "## The categorical variables were not important at all in the model, using aggregates such as the mode.  This function instead computes the proportion of occurence for the unique ID, and improved the model much more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_interactions(df,colstart):\n",
    "    column_names=df[df.columns[colstart:]].select_dtypes(include=[np.int64,np.float64]).columns\n",
    "    subcolumns_exclude=[]\n",
    "    for col in column_names:\n",
    "        subcolumns_exclude.append(col)\n",
    "        for subcol in column_names:\n",
    "            if subcol not in subcolumns_exclude:\n",
    "                df[col+'*'+subcol]=df[col]*df[subcol]\n",
    "                df[col+'/'+subcol]=df[col]/df[subcol]\n",
    "                df[col+'+'+subcol]=df[col]+df[subcol]\n",
    "                df[col+'-'+subcol]=df[col]-df[subcol]\n",
    "    return df\n",
    "\n",
    "def correlated_columns(df,threshold):\n",
    "\n",
    "    corr_matrix = df.corr().abs()\n",
    "\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    print('There are %d total columns.' % (len(df.columns)))\n",
    "    print('There are %d columns to remove.' % (len(to_drop)))\n",
    "    \n",
    "    return to_drop\n",
    "\n",
    "def aggregate_category_freq(df,df_main,df_name,joinID):\n",
    "\n",
    "    for col in df.select_dtypes(include=[object]).columns:\n",
    "        for cat in df[col].drop_duplicates().dropna():\n",
    "            catcol_aggs=pd.DataFrame((df[df[col]==cat].groupby(joinID)[col].agg('count')/df.groupby(joinID)[col].agg('count')).fillna(0))\n",
    "            catcol_aggs.columns=[df_name+'.'+col+'_'+cat]\n",
    "            df_main=df_main.join(catcol_aggs[df_name+'.'+col+'_'+cat],how='left')\n",
    "        \n",
    "    return df_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare the entityset using featuretools package.  This is how all the subtables and main table will be combined to interact with one another when creating aggregate features.\n",
    "\n",
    "## Variables coded as numbers, specifically here \"0,1\", needs to be specified as categorical so the correct aggregate columns are created.  For example, we should take the mode (maybe \"1\", instead of the mean (maybe .712) of a binary column of 0s and 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = ft.EntitySet('loan_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read previous application dataset, filter, replace bad data (\"365243\" is an extreme outlier in all of these columns, concluding that this is probably how one of the data sources defines their NA fields)\n",
    "\n",
    "## Use functions declared earlier to create interaction terms and drop intercorrelated ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREVIOUS_APPLICATION = pd.read_csv(\"previous_application.csv\",na_values=['XNA','XAP'])\n",
    "PREVIOUS_APPLICATION=PREVIOUS_APPLICATION[PREVIOUS_APPLICATION.SK_ID_CURR.isin(keys)]\n",
    "col_count=len(PREVIOUS_APPLICATION.columns)\n",
    "\n",
    "for col in ['DAYS_LAST_DUE','DAYS_TERMINATION','DAYS_FIRST_DRAWING','DAYS_FIRST_DUE','DAYS_LAST_DUE_1ST_VERSION']:\n",
    "    PREVIOUS_APPLICATION[col].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "PREVIOUS_APPLICATION=pd.concat([PREVIOUS_APPLICATION[PREVIOUS_APPLICATION.columns[:2]],\n",
    "                                PREVIOUS_APPLICATION[PREVIOUS_APPLICATION[PREVIOUS_APPLICATION.columns[2:]].count().sort_values(ascending=False).index]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREVIOUS_APPLICATION=generate_interactions(PREVIOUS_APPLICATION,2)\n",
    "PREVIOUS_APPLICATION.dropna(how='all',axis=1,inplace=True)\n",
    "PREVIOUS_APPLICATION=pd.concat([PREVIOUS_APPLICATION[PREVIOUS_APPLICATION.columns[:col_count]],\n",
    "                                PREVIOUS_APPLICATION[PREVIOUS_APPLICATION[PREVIOUS_APPLICATION.columns[col_count:]].count().sort_values(ascending=False).index]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 709 total columns.\n",
      "There are 524 columns to remove.\n"
     ]
    }
   ],
   "source": [
    "PREVIOUS_APPLICATION.drop(correlated_columns(PREVIOUS_APPLICATION,.95),axis=1,inplace=True)\n",
    "APPLICATION_TRAIN=aggregate_category_freq(PREVIOUS_APPLICATION,APPLICATION_TRAIN,'PREVIOUS_APPLICATION','SK_ID_CURR')\n",
    "PREVIOUS_APPLICATION.drop(PREVIOUS_APPLICATION.select_dtypes(include=[object]).columns,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add previous application dataset to entityset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "371"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.entity_from_dataframe(\n",
    "    entity_id = 'PREVIOUS_APPLICATION', \n",
    "    dataframe = PREVIOUS_APPLICATION,\n",
    "    index = 'SK_ID_PREV',\n",
    ")\n",
    "\n",
    "del PREVIOUS_APPLICATION; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This process is repetitive, same process is done for all sub tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_CASH_BALANCE = pd.read_csv(\"POS_CASH_balance.csv\")\n",
    "POS_CASH_BALANCE=POS_CASH_BALANCE[POS_CASH_BALANCE.SK_ID_CURR.isin(keys)]\n",
    "col_count=len(POS_CASH_BALANCE.columns)\n",
    "POS_CASH_BALANCE=pd.concat([POS_CASH_BALANCE[POS_CASH_BALANCE.columns[:2]],\n",
    "                            POS_CASH_BALANCE[POS_CASH_BALANCE[POS_CASH_BALANCE.columns[2:]].count().sort_values(ascending=False).index]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_CASH_BALANCE=generate_interactions(POS_CASH_BALANCE,2)\n",
    "POS_CASH_BALANCE.dropna(how='all',axis=1,inplace=True)\n",
    "POS_CASH_BALANCE=pd.concat([POS_CASH_BALANCE[POS_CASH_BALANCE.columns[:col_count]],\n",
    "                            POS_CASH_BALANCE[POS_CASH_BALANCE[POS_CASH_BALANCE.columns[col_count:]].count().sort_values(ascending=False).index]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 48 total columns.\n",
      "There are 18 columns to remove.\n"
     ]
    }
   ],
   "source": [
    "POS_CASH_BALANCE.drop(correlated_columns(POS_CASH_BALANCE,.95),axis=1,inplace=True)\n",
    "APPLICATION_TRAIN=aggregate_category_freq(POS_CASH_BALANCE,APPLICATION_TRAIN,'POS_CASH_BALANCE','SK_ID_CURR')\n",
    "POS_CASH_BALANCE.drop(POS_CASH_BALANCE.select_dtypes(include=[object]).columns,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.entity_from_dataframe(\n",
    "    entity_id = 'POS_CASH_BALANCE', \n",
    "    dataframe = POS_CASH_BALANCE,\n",
    "    make_index = True,\n",
    "    index='index',\n",
    ")\n",
    "\n",
    "del POS_CASH_BALANCE; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREDIT_CARD_BALANCE = pd.read_csv(\"credit_card_balance.csv\")\n",
    "CREDIT_CARD_BALANCE=CREDIT_CARD_BALANCE[CREDIT_CARD_BALANCE.SK_ID_CURR.isin(keys)]\n",
    "col_count=len(CREDIT_CARD_BALANCE.columns)\n",
    "CREDIT_CARD_BALANCE=pd.concat([CREDIT_CARD_BALANCE[CREDIT_CARD_BALANCE.columns[:2]],\n",
    "                               CREDIT_CARD_BALANCE[CREDIT_CARD_BALANCE[CREDIT_CARD_BALANCE.columns[2:]].count().sort_values(ascending=False).index]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREDIT_CARD_BALANCE=generate_interactions(CREDIT_CARD_BALANCE,2)\n",
    "CREDIT_CARD_BALANCE.dropna(how='all',axis=1,inplace=True)\n",
    "#CREDIT_CARD_BALANCE=pd.concat([CREDIT_CARD_BALANCE[CREDIT_CARD_BALANCE.columns[:col_count]],\n",
    "#                               CREDIT_CARD_BALANCE[CREDIT_CARD_BALANCE[CREDIT_CARD_BALANCE.columns[col_count:]].count().sort_values(ascending=False).index]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 783 total columns.\n",
      "There are 517 columns to remove.\n"
     ]
    }
   ],
   "source": [
    "CREDIT_CARD_BALANCE.drop(correlated_columns(CREDIT_CARD_BALANCE,.95),axis=1,inplace=True)\n",
    "APPLICATION_TRAIN=aggregate_category_freq(CREDIT_CARD_BALANCE,APPLICATION_TRAIN,'CREDIT_CARD_BALANCE','SK_ID_CURR')\n",
    "CREDIT_CARD_BALANCE.drop(CREDIT_CARD_BALANCE.select_dtypes(include=[object]).columns,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.entity_from_dataframe(\n",
    "    entity_id = 'CREDIT_CARD_BALANCE', \n",
    "    dataframe = CREDIT_CARD_BALANCE,\n",
    "    make_index = True,\n",
    "    index='index',\n",
    ")\n",
    "\n",
    "del CREDIT_CARD_BALANCE; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTALLMENTS_PAYMENTS = pd.read_csv(\"installments_payments.csv\")\n",
    "INSTALLMENTS_PAYMENTS=INSTALLMENTS_PAYMENTS[INSTALLMENTS_PAYMENTS.SK_ID_CURR.isin(keys)]\n",
    "col_count=len(INSTALLMENTS_PAYMENTS.columns)\n",
    "INSTALLMENTS_PAYMENTS=pd.concat([INSTALLMENTS_PAYMENTS[INSTALLMENTS_PAYMENTS.columns[:2]],\n",
    "                               INSTALLMENTS_PAYMENTS[INSTALLMENTS_PAYMENTS[INSTALLMENTS_PAYMENTS.columns[2:]].count().sort_values(ascending=False).index]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTALLMENTS_PAYMENTS=generate_interactions(INSTALLMENTS_PAYMENTS,2)\n",
    "INSTALLMENTS_PAYMENTS.dropna(how='all',axis=1,inplace=True)\n",
    "INSTALLMENTS_PAYMENTS=pd.concat([INSTALLMENTS_PAYMENTS[INSTALLMENTS_PAYMENTS.columns[:col_count]],\n",
    "                                 INSTALLMENTS_PAYMENTS[INSTALLMENTS_PAYMENTS[INSTALLMENTS_PAYMENTS.columns[col_count:]].count().sort_values(ascending=False).index]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 68 total columns.\n",
      "There are 39 columns to remove.\n"
     ]
    }
   ],
   "source": [
    "INSTALLMENTS_PAYMENTS.drop(correlated_columns(INSTALLMENTS_PAYMENTS,.95),axis=1,inplace=True)\n",
    "APPLICATION_TRAIN=aggregate_category_freq(INSTALLMENTS_PAYMENTS,APPLICATION_TRAIN,'INSTALLMENTS_PAYMENTS','SK_ID_CURR')\n",
    "INSTALLMENTS_PAYMENTS.drop(INSTALLMENTS_PAYMENTS.select_dtypes(include=[object]).columns,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.entity_from_dataframe(\n",
    "    entity_id = 'INSTALLMENTS_PAYMENTS', \n",
    "    dataframe = INSTALLMENTS_PAYMENTS,\n",
    "    make_index = True,\n",
    "    index='index',\n",
    ")\n",
    "\n",
    "del INSTALLMENTS_PAYMENTS; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUREAU=pd.read_csv(\"bureau.csv\",na_values=['XNA','XAP'])\n",
    "BUREAU=BUREAU[BUREAU.SK_ID_CURR.isin(keys)]\n",
    "bureaukeys=BUREAU.SK_ID_BUREAU\n",
    "col_count=len(BUREAU.columns)\n",
    "BUREAU=pd.concat([BUREAU[BUREAU.columns[:2]],\n",
    "                  BUREAU[BUREAU[BUREAU.columns[2:]].count().sort_values(ascending=False).index]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUREAU=generate_interactions(BUREAU,2)\n",
    "BUREAU.dropna(how='all',axis=1,inplace=True)\n",
    "BUREAUS=pd.concat([BUREAU[BUREAU.columns[:col_count]],\n",
    "                   BUREAU[BUREAU[BUREAU.columns[col_count:]].count().sort_values(ascending=False).index]],axis=1)\n",
    "BUREAU=BUREAU.set_index('SK_ID_BUREAU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 280 total columns.\n",
      "There are 153 columns to remove.\n"
     ]
    }
   ],
   "source": [
    "BUREAU.drop(correlated_columns(BUREAU,.95),axis=1,inplace=True)\n",
    "APPLICATION_TRAIN=aggregate_category_freq(BUREAU,APPLICATION_TRAIN,'BUREAU','SK_ID_CURR')\n",
    "BUREAU.drop(BUREAU.select_dtypes(include=[object]).columns,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUREAU_BALANCE = pd.read_csv(\"bureau_balance.csv\",na_values=['X'])\n",
    "BUREAU_BALANCE=BUREAU_BALANCE[BUREAU_BALANCE.SK_ID_BUREAU.isin(bureaukeys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUREAU=aggregate_category_freq(BUREAU_BALANCE,BUREAU,'BUREAU_BALANCE','SK_ID_BUREAU')\n",
    "BUREAU_BALANCE.drop(BUREAU_BALANCE.select_dtypes(include=[object]).columns,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUREAU_BALANCE table contains the status for each month in DPD. \n",
    "\n",
    "## Many of the statuses were blank (Status of \"X\"), so flash fill method was used to fill in the previous month's status of the of loan for each NA.  The model performed better leaving these as NA, so we abandoned this strategy.  \n",
    "\n",
    "# The status column is categorical, we tried converting this to numeric by replacing \"C\" (completed) status by 0, but leaving as categorical was best for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUREAU.reset_index(level=0,inplace=True)\n",
    "es.entity_from_dataframe(\n",
    "    entity_id = 'BUREAU', \n",
    "    dataframe = BUREAU,\n",
    "    index = 'SK_ID_BUREAU',\n",
    ")\n",
    "\n",
    "\n",
    "del BUREAU; gc.collect()\n",
    "\n",
    "\n",
    "es.entity_from_dataframe(\n",
    "    entity_id = 'BUREAU_BALANCE', \n",
    "    dataframe = BUREAU_BALANCE,\n",
    "    make_index=True,\n",
    "    index='index',\n",
    ")\n",
    "\n",
    "\n",
    "del BUREAU_BALANCE; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: loan_data\n",
       "  Entities:\n",
       "    PREVIOUS_APPLICATION [Rows: 234942, Columns: 169]\n",
       "    POS_CASH_BALANCE [Rows: 1321544, Columns: 30]\n",
       "    CREDIT_CARD_BALANCE [Rows: 488676, Columns: 266]\n",
       "    INSTALLMENTS_PAYMENTS [Rows: 1813984, Columns: 30]\n",
       "    BUREAU [Rows: 232790, Columns: 132]\n",
       "    BUREAU_BALANCE [Rows: 2202742, Columns: 3]\n",
       "    APPLICATION_TRAIN [Rows: 49650, Columns: 286]\n",
       "  Relationships:\n",
       "    PREVIOUS_APPLICATION.SK_ID_CURR -> APPLICATION_TRAIN.SK_ID_CURR\n",
       "    BUREAU.SK_ID_CURR -> APPLICATION_TRAIN.SK_ID_CURR\n",
       "    POS_CASH_BALANCE.SK_ID_CURR -> APPLICATION_TRAIN.SK_ID_CURR\n",
       "    CREDIT_CARD_BALANCE.SK_ID_CURR -> APPLICATION_TRAIN.SK_ID_CURR\n",
       "    INSTALLMENTS_PAYMENTS.SK_ID_CURR -> APPLICATION_TRAIN.SK_ID_CURR\n",
       "    BUREAU_BALANCE.SK_ID_BUREAU -> BUREAU.SK_ID_BUREAU"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APPLICATION_TRAIN.reset_index(level=0,inplace=True)\n",
    "es.entity_from_dataframe(\n",
    "    entity_id='APPLICATION_TRAIN',\n",
    "    dataframe=APPLICATION_TRAIN.drop('TARGET', axis=1),\n",
    "    index='SK_ID_CURR',\n",
    "    variable_types={\n",
    "        f: ft.variable_types.Categorical \n",
    "        for f in APPLICATION_TRAIN.columns if f.startswith('FLAG_')\n",
    "    }\n",
    ")\n",
    "\n",
    "del APPLICATION_TRAIN; gc.collect()\n",
    "\n",
    "\n",
    "relationship1 = ft.Relationship(\n",
    "    es['APPLICATION_TRAIN']['SK_ID_CURR'],\n",
    "    es['PREVIOUS_APPLICATION']['SK_ID_CURR']\n",
    ")\n",
    "\n",
    "relationship2 = ft.Relationship(\n",
    "    es['APPLICATION_TRAIN']['SK_ID_CURR'],\n",
    "    es['BUREAU']['SK_ID_CURR']\n",
    ")\n",
    "\n",
    "relationship3 = ft.Relationship(\n",
    "    es['APPLICATION_TRAIN']['SK_ID_CURR'],\n",
    "    es['POS_CASH_BALANCE']['SK_ID_CURR']\n",
    ")\n",
    "\n",
    "relationship4 = ft.Relationship(\n",
    "    es['APPLICATION_TRAIN']['SK_ID_CURR'],\n",
    "    es['CREDIT_CARD_BALANCE']['SK_ID_CURR']\n",
    ")\n",
    "\n",
    "\n",
    "relationship5 = ft.Relationship(\n",
    "    es['APPLICATION_TRAIN']['SK_ID_CURR'],\n",
    "    es['INSTALLMENTS_PAYMENTS']['SK_ID_CURR']\n",
    ")\n",
    "\n",
    "relationship6 = ft.Relationship(\n",
    "    es['BUREAU']['SK_ID_BUREAU'],\n",
    "    es['BUREAU_BALANCE']['SK_ID_BUREAU']\n",
    ")\n",
    "\n",
    "es.add_relationship(relationship1)\n",
    "es.add_relationship(relationship2)\n",
    "es.add_relationship(relationship3)\n",
    "es.add_relationship(relationship4)\n",
    "es.add_relationship(relationship5)\n",
    "es.add_relationship(relationship6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below we are creating the feature matrix (fm) that adds in aggregate features defined below for all sub tables.  \n",
    "\n",
    "## For all numeric features, we'll add the instance count, mean, median, skew, sum, standard deviation, max, and min.  \n",
    "\n",
    "## categorical features were converted to numeric proportions (explained eariler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 4667 features\n",
      "Elapsed: 2:06:48 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 13/13 chunks \n"
     ]
    }
   ],
   "source": [
    "fm, feature_defs = ft.dfs(\n",
    "    entityset=es, \n",
    "    target_entity=\"APPLICATION_TRAIN\", \n",
    "    agg_primitives=[\n",
    "        #\"avg_time_between\",\n",
    "        #\"time_since_last\",\n",
    "        \"mean\",\n",
    "        \"median\",\n",
    "        #\"num_unique\",\n",
    "        \"count\",\n",
    "        \"skew\",\n",
    "        \"sum\",\n",
    "        \"std\",\n",
    "        #\"mode\",\n",
    "        \"max\",\n",
    "        \"min\"\n",
    "    ],\n",
    "    trans_primitives=[\n",
    "        #\"time_since_previous\", \n",
    "        #\"cum_mean\",\n",
    "        #\"cum_max\",\n",
    "        #\"cum_min\"\n",
    "        #\"percentile\",\n",
    "        #\"add\"\n",
    "    ],\n",
    "    max_depth=2,\n",
    "    #cutoff_time=cutoff_times,\n",
    "    training_window=ft.Timedelta(60, \"d\"),\n",
    "    max_features=5000,\n",
    "    chunk_size=4000,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As this is a long process, will save to a .pkl file for use in the modeling notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.to_pickle('loan data.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
